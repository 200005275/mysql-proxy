MySQL Proxy
===========

C10k
----

  The MySQL Proxy network engine is meant to handle several thousands connections at the same time. We 
want to use it for load-balancing and fail-over which means we have to handle the connections for
a larger group of MySQL backend servers nicely. We aim for 5k to 10k connections.
  Up to MySQL Proxy 0.7 we use a pure event-driven, non-blocking networking approach is described in
http://kegel.com/c10k.html#nb using libevent 1.4.x. 
  A event-driven design has a very small foot-print for idling connections: we just store the
connection state and let it wait for a event. 

Scripting
---------

  Flexibility is key. From the start MySQL Proxy was meant to be extensible and customizable. We embedded
a scripting language to control how the proxy core shall handle the processing of packets. You can 
change the packets, reject packets or event inject packets into the network stream. The main work,
the protocol handling, ... is all hidden in the core, the scripting layer only controls how the core
shall operate.

Threaded IO
-----------

  In MySQL 0.8 we added threaded network-io to allow the proxy to scale out with the numbers of CPUs 
and network cards available.

To enable network-threading you just start the proxy with:

  --event-threads={2 * no-of-cores} (default: 0)

A event-thread is a simple small thread around "event_base_dispatch()" which on a network- or time-event
executes our core functions. These threads either execute the core functions or idle. If they idle 
they can read new events to wait for and add them to their wait-list.

A connection can jump between event-threads: the idling event-thread is taking the wait-for-event
request and executes the code. Whenever the connection has to wait for a event again it is unregister
itself from the thread, send its wait-for-event request to the global event-queue again.

  Up to MySQL Proxy 0.8 the execution of the scripting code is single-threaded: a global mutex protects
the plugin interface. As a connection is either sending packets or calling plugin function the network 
events will be handled in parallel and only wait if several connections want to call a plugin function

Implementation
..............

In chassis-event-thread.c the chassis_event_thread_loop() is the event-thread itself. It gets setup by
chassis_event_threads_init_thread() which registers itself to a pipe(), a classic simple pipe. That
pipe is a common hack in libevent to map any kind of event to a the fd-based event-handlers like poll:

  * the event_base_dispatch() blocks until a fd-event triggers
  * timers, signals, ... can't interrupt event_base_dispatch() directly
  * instead they cause a write(pipe_fd, ".", 1); which triggers a fd-event
    which afterwards gets handled

In chassis-event-thread.c we use the pipe to signal that something is in the global event-queue to be
processed by one of the event-threads ... see chassis_event_handle(). All idling threads will process
that even and will pull from the event queue in parallel to add the event to their events to listen for.

To add a event to the event-queue you can call chassis_event_add() or chassis_event_add_local(). See 
the source when to use which.

Threaded Scripting
------------------

Usually the scripts are small and only make simple decisions leaving most of the work to the network layer.
In 0.9 we will make the scripting layer multi-threaded allow several scripting threads at the same time,
working from a small pool threads.

That will allow the scripting layer to call blocking or slow functions without infecting the execution of
other connections.

Lifting the global plugin mutex will mean we have to handle access to global structure differently. Most 
of the access is happening on connection-level (the way we do the event-threading) and only access to
global structures like "proxy.global.*" has to synchronized. For that we will look into using Lua lanes
to send data around between independent Lua-states.



  
